{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OOeAf6ca__G1"
      },
      "outputs": [],
      "source": [
        "# Core TensorFlow and Keras imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import EfficientNetB0\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "data_augmentation = tf.keras.Sequential([\n",
        "    layers.RandomFlip(\"horizontal\"),\n",
        "    layers.RandomRotation(0.1),\n",
        "    layers.RandomZoom(0.1),\n",
        "])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "okauyGq2A43U",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Load CIFAR-10 dataset\n",
        "# CIFAR-10 contains 60,000 32x32 RGB images across 10 classes\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "# Convert class labels to one-hot encoded vectors\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Preprocessing function:\n",
        "# - Resize images to 224x224 to match EfficientNet input requirements\n",
        "# - Normalize pixel values to [0, 1]\n",
        "def preprocess(image, label):\n",
        "    image = tf.image.resize(image, (224,224))\n",
        "    image = image / 255.0\n",
        "    return image, label\n",
        "\n",
        "# Create TensorFlow Dataset for efficient input pipeline\n",
        "\n",
        "train_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_ds = train_ds.map(preprocess).shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "test_ds = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "test_ds = test_ds.map(preprocess).batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s1lJs6gtBqNO"
      },
      "outputs": [],
      "source": [
        "# Baseline CNN model trained from scratch (no pre-trained knowledge)\n",
        "# Acts as a performance reference point\n",
        "baseline_model = models.Sequential([\n",
        "    layers.Conv2D(32, 3, activation='relu', input_shape=(224,224,3)),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Conv2D(64, 3, activation='relu'),\n",
        "    layers.MaxPooling2D(),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(128, activation='relu'),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "baseline_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "baseline_history = baseline_model.fit(\n",
        "    train_ds,  # Use the preprocessed dataset\n",
        "    epochs=10,\n",
        "    validation_data=test_ds # Use the preprocessed test dataset for validation\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0lkW-du-GTDW"
      },
      "outputs": [],
      "source": [
        "# Load EfficientNetB0 pre-trained on ImageNet\n",
        "# include_top=False removes the original ImageNet classifier\n",
        "\n",
        "\n",
        "\n",
        "base_model = EfficientNetB0(\n",
        "    weights='imagenet',\n",
        "    include_top=False,\n",
        "    input_shape=(224,224,3)\n",
        ")\n",
        "\n",
        "# Freeze all layers of the pre-trained model\n",
        "# This prevents updating ImageNet-learned weights\n",
        "base_model.trainable = False\n",
        "\n",
        "tl_model = models.Sequential([\n",
        "    data_augmentation,      # Agumentation\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.4),    # regularization\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "tl_model.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p014gINHGWvK"
      },
      "outputs": [],
      "source": [
        "# Train only the newly added classifier layers\n",
        "tl_frozen_history = tl_model.fit(\n",
        "    train_ds,\n",
        "    epochs=5,\n",
        "    validation_data=test_ds\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_U-19Y5Imz4"
      },
      "outputs": [],
      "source": [
        "base_model.trainable = True\n",
        "\n",
        "# Re-create the model after changing trainable status\n",
        "# This ensures the model graph is properly constructed with the unfrozen base_model\n",
        "tl_model = models.Sequential([\n",
        "    data_augmentation,\n",
        "    base_model,\n",
        "    layers.GlobalAveragePooling2D(),\n",
        "    layers.Dense(256, activation='relu'),\n",
        "    layers.Dropout(0.4),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "tl_model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(1e-6),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Recompile with a very small learning rate\n",
        "# This avoids destroying pre-trained features (catastrophic forgetting)\n",
        "tl_finetune_history = tl_model.fit(\n",
        "    train_ds,\n",
        "    epochs=15,\n",
        "    validation_data=test_ds,\n",
        "    callbacks=[early_stop]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-6aWY0m0MsuT"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "# Baseline\n",
        "plt.plot(baseline_history.history['accuracy'], label='Baseline Train')\n",
        "plt.plot(baseline_history.history['val_accuracy'], label='Baseline Val')\n",
        "\n",
        "# Transfer Learning - Frozen\n",
        "#plt.plot(tl_frozen_history.history['accuracy'], label='TL Train (Frozen)')\n",
        "#plt.plot(tl_frozen_history.history['val_accuracy'], label='TL Val (Frozen)')\n",
        "\n",
        "# Transfer Learning - Fintetuned\n",
        "plt.plot(tl_finetune_history.history['accuracy'], label='TL Train (Frozen)')\n",
        "plt.plot(tl_finetune_history.history['val_accuracy'], label='TL Val (Frozen)')\n",
        "\n",
        "# Fine-tuned accuracy (continued epochs)\n",
        "#fine_tune_epochs = range(\n",
        "    #len(tl_frozen_history.history['accuracy']),\n",
        "    #len(tl_frozen_history.history['accuracy']) + len(tl_finetune_history.history['accuracy'])\n",
        "#)\n",
        "\n",
        "#plt.plot(fine_tune_epochs, tl_finetune_history.history['accuracy'],\n",
        "#         label='TL Train (Fine-tuned)')\n",
        "\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Baseline vs Transfer Learning Performance\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H944GrOoMa95"
      },
      "outputs": [],
      "source": [
        "baseline_test_acc = baseline_model.evaluate(test_ds, verbose=0)[1]\n",
        "tl_test_acc = tl_model.evaluate(test_ds, verbose=0)[1]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.bar(['Baseline CNN', 'Transfer Learning'],\n",
        "        [baseline_test_acc, tl_test_acc])\n",
        "plt.title(\"Test Accuracy Comparison\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-ytHMzbPpxSc"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "\n",
        "def plot_confusion_matrix(model, dataset, title):\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    for images, labels in dataset:\n",
        "        preds = model.predict(images, verbose=0)\n",
        "        y_pred.extend(np.argmax(preds, axis=1))\n",
        "        y_true.extend(np.argmax(labels.numpy(), axis=1))\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
        "    disp.plot()\n",
        "    plt.title(title)\n",
        "    plt.show()\n",
        "\n",
        "plot_confusion_matrix(baseline_model, test_ds, \"Baseline CNN Confusion Matrix\")\n",
        "plot_confusion_matrix(tl_model, test_ds, \"Transfer Learning Confusion Matrix\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECILDek-wuM-"
      },
      "outputs": [],
      "source": [
        "### Key Observation\n",
        "# 1. Transfer learning can lead to overfitting on small datasets.\n",
        "# 2. Data augmentation (flips, rotations, zoom) helps generalize the model.\n",
        "# 3. Adding dropout layers in the fully connected part reduces overfitting.\n",
        "# 4. Pre-trained models require careful fine-tuning to avoid memorizing training data.\n",
        "# 5. Early stopping can further help prevent overfitting."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}